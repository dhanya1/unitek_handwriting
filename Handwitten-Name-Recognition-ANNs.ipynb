{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten name recognition using ANNs\n",
    "\n",
    "### Made by Iker Garc√≠a and Eritz Yerga\n",
    "https://github.com/ikergarcia1996/Handwritten-Names-Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before beginning we will define how we want our program to work.\n",
    "\n",
    "Change any of these parameters if you want the notebook to function in a different way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define if you want to download data from the original database or use the dataset one already provided and preprocessed\n",
    "# Use:\n",
    "# 'load': If you want to load the datase from the directory\n",
    "# 'download': To download data from the database and process the images\n",
    "dataset_load_method = 'load'\n",
    "\n",
    "# Define if you want to save the dataset to a file\n",
    "save_dataset = False\n",
    "\n",
    "# Define if you want to load the trained classifiers from the directory\n",
    "load_classifiers = False\n",
    "\n",
    "# Define if you want to save the trained classifiers to a file\n",
    "save_classifiers = True\n",
    "\n",
    "# Define if you want to save classification test output to a file\n",
    "save_results = True\n",
    "if (save_results):\n",
    "    result_output_file = open('result_output.txt','w') \n",
    "\n",
    "# Define if you want to print errors and warnings\n",
    "enable_error_output = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Library imports\n",
    "\n",
    "We import the needed libraries to run our code. All of these libraries are installable through pip3 except cv2 (_see note bellow_).\n",
    "\n",
    "*NOTE:* To import _cv2_ you might need to install the _python3-opencv_ package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system('pip install plotly')\n",
    "os.system('pip install opencv-contrib-python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5d981689f94d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import scipy\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn import linear_model, datasets, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.neural_network import BernoulliRBM, MLPClassifier\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from skimage import data, color, exposure, measure\n",
    "from skimage.transform import resize\n",
    "from skimage.feature import hog\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.externals import joblib\n",
    "import urllib\n",
    "from io import StringIO\n",
    "import cv2\n",
    "from scipy import ndimage\n",
    "from difflib import SequenceMatcher\n",
    "from sys import stdout\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also define a small function for displaying purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_percentage(prct, msg=None):\n",
    "    if (prct > 100 or prct < 0):\n",
    "        return\n",
    "    clear_output(wait=True)\n",
    "    if (msg == None):\n",
    "        stdout.write(\"Progress: [\")\n",
    "    else:\n",
    "        stdout.write(msg+\" [\")\n",
    "    end = int(int(prct)/10)\n",
    "    for i in range(0, end):\n",
    "        stdout.write(\"=\")\n",
    "    for i in range(end, 10):\n",
    "        stdout.write(\" \")\n",
    "    stdout.write(\"] \"+str(prct)+\"%\")\n",
    "    stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Importing the dataset with the urls and names\n",
    "\n",
    "We import the \"Transcriptions of handwriten names\" database obtained from https://www.crowdflower.com/data-for-everyone/ . As the csv file in the website has some errors, we fixed those errors using a small program written in Java (see java file included in source: database_fix.java)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('first_and_last_names_fix.csv', sep=',',header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to create the full dataset and labels\n",
    "\n",
    "As the database _df_ we imported doesn't contain the images but link to the corresponding images, we need to download them and create a dataset with those and the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *_delborders_* function eliminates the bottom border noise in case of \"first_b\" and \"last_b\" types in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are applying filters and making it clean.\n",
    "\n",
    "def delborders(crop):\n",
    "    cropf = ndimage.gaussian_filter(crop, 0.5)\n",
    "    cropbin = (cropf<0.8)\n",
    "    labeled, nr_objects = ndimage.label(cropbin)\n",
    "    labels_to_delete = []\n",
    "    for i in range(0, labeled.shape[1]):\n",
    "        if (labeled[labeled.shape[0]-1][i] > 0):\n",
    "            labels_to_delete.append(labeled[labeled.shape[0]-1][i])\n",
    "    \n",
    "    label_in_delete = False\n",
    "    for x in range(0, labeled.shape[1]):\n",
    "        for y in range(0, labeled.shape[0]):\n",
    "            label_in_delete = False\n",
    "            for l in range(0, len(labels_to_delete)):\n",
    "                if (labeled[y][x] == labels_to_delete[l]):\n",
    "                    label_in_delete = True\n",
    "            \n",
    "            if(label_in_delete):\n",
    "                crop[y][x] = 1.0\n",
    "    \n",
    "    return crop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *_getcrop_* function obtains an image from an entry of the database and returns the image in grayscale and cropped to show just the name part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getcrop(n):\n",
    "    try: \n",
    "        urllib.request.urlretrieve(df[1][n], \"temp.jpg\")\n",
    "    except urllib.error.URLError as e:\n",
    "        return None, False\n",
    "    img = cv2.imread('temp.jpg')\n",
    "    imgh, imgw = img.shape[:-1]\n",
    "    img_rgb = img.copy()\n",
    "    template = cv2.imread('template.png')\n",
    "    h, w = template.shape[:-1]\n",
    "\n",
    "    template_match_success = False\n",
    "    res = cv2.matchTemplate(img_rgb, template, cv2.TM_CCOEFF_NORMED)\n",
    "    threshold = .7\n",
    "    loc = np.where(res >= threshold)\n",
    "    for pt in zip(*loc[::-1]):\n",
    "        print(pt)# Switch collumns and rows\n",
    "        cv2.rectangle(img_rgb, pt, (pt[0] + w, pt[1] + h), (0, 0, 255), 2)\n",
    "        croph1 = pt[1]\n",
    "        croph2 = pt[1]+h\n",
    "        cropw = pt[0] + w\n",
    "        template_match_success = True\n",
    "\n",
    "    if (not template_match_success):\n",
    "        #Template matching has failed so return...\n",
    "        return img, False\n",
    "\n",
    "    if (df[3][n] == 'first' or df[3][n] == 'last'):\n",
    "        crop = img.copy()[max(croph1-6, 0):min(croph2+6, imgh), cropw:imgw]\n",
    "    else:\n",
    "        crop = img.copy()[max(min(croph2+4, imgh-1), 0):imgh, :]\n",
    "        \n",
    "    crop = color.rgb2gray(crop)\n",
    "    if (df[3][n] == 'first_b' or df[3][n] == 'last_b'):\n",
    "        crop = delborders(crop)\n",
    "    return crop, True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the gen_dataset function, this function creates a binarized image list \"_data_\" and a label list \"_labels_\" from the first _n_ entries of the database (if not specified uses all the entries of the database) using the crops of the _getcrop_ function and its corresponding label on that index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dataset(n=df.shape[0]):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for i in range(1, n):\n",
    "        crop, success = getcrop(i)\n",
    "        if (success):\n",
    "            data.append(crop)\n",
    "            labels.append(df[2][i])\n",
    "        else:\n",
    "            if (enable_error_output):\n",
    "                print(\"[WARNING] Template matching has failed for image: \"+str(i))\n",
    "        print_percentage((i*100/(n-1)), \"Fetched \"+str(i)+\" images:\")\n",
    "    \n",
    "    print_percentage(100, \"Fetched \"+str(n-1)+\" images:\")\n",
    "    print(\"\")\n",
    "    print(\"Finished!\")\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining the dataset\n",
    "\n",
    "Now we can download and create the whole dataset and its corresponding label list or load the dataset from the saved files. The process takes a while to complete..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Obtain dataset from original data if defined (we can define how much data of the \n",
    "# original database we want to use to the function or leave it blank to use all the data)\n",
    "if (dataset_load_method == 'download'):\n",
    "    dataset, labels = gen_dataset(10000)\n",
    "\n",
    "# Load dataset from files\n",
    "if (dataset_load_method == 'load'):\n",
    "    dataset = np.load(\"HandwrittenNames_data.npz\")['data']\n",
    "    labels = np.load(\"HandwrittenNames_labels.npz\")['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If specified, the generated dataset can be saved to .npz files using these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset to a file if defined\n",
    "if (save_dataset):\n",
    "    np.savez(\"HandwrittenNames_data.npz\", data=dataset)\n",
    "    np.savez(\"HandwrittenNames_labels.npz\", data=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot some images and print their corresponding labels to check that everything is correct: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change selection to plot a different image and label\n",
    "selection = 0\n",
    "plt.imshow(dataset[selection], cmap='gray')\n",
    "plt.show()\n",
    "print(labels[selection])\n",
    "print(str(type(labels[0])))\n",
    "print(type(dataset[selection]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Defining extra helpful functions\n",
    "\n",
    "In this section of the notebook we will define some functions that will be useful later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_labels\n",
    "\n",
    "This function labels the connected components in an image by binarizing it and running a clustering method, it returns the labels and the number of labels it detects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(crop):\n",
    "    img = crop.copy() # gray-scale image\n",
    "\n",
    "    # You could smooth the image (to remove small objects) but we saw better results without using it...\n",
    "    # blur_radius = 0.5\n",
    "    # imgf = ndimage.gaussian_filter(img, blur_radius)\n",
    "\n",
    "    threshold = 0.8\n",
    "\n",
    "    # Find connected components\n",
    "    labeled, nr_objects = ndimage.label(img<threshold) \n",
    "    #print(\"Number of objects is \" +str(nr_objects))\n",
    "\n",
    "    return labeled, nr_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_bboxes\n",
    "\n",
    "This function gets the bounding boxes to cut each character correctly given the labels obtained from get_labels. It returns a list of each character's bounding boxes (2 2D points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bboxes(labeled, nr_objects):\n",
    "    bboxes = np.zeros((nr_objects, 2, 2), dtype='int')\n",
    "\n",
    "    x1, y1, x2, y2 = 0, labeled.shape[0], 0, 0\n",
    "    coord = 0\n",
    "    cont = 0\n",
    "    ytop, ybot = 0, 0\n",
    "    nzero, firstb = False, False\n",
    "\n",
    "    for x in range(0, labeled.shape[1]):\n",
    "        nzero, firstb = False, False\n",
    "        ytop, ybot = 0, 0\n",
    "        for y in range(0, labeled.shape[0]):\n",
    "            if (labeled[y][x] > 0):\n",
    "                nzero = True\n",
    "                if (not firstb):\n",
    "                    ytop = y\n",
    "                    firstb = True\n",
    "                ybot = y\n",
    "\n",
    "        if (nzero):\n",
    "            if (ytop < y1):\n",
    "                y1 = ytop\n",
    "            if (ybot > y2):\n",
    "                y2 = ybot\n",
    "            if (coord == 0):\n",
    "                x1 = x\n",
    "                coord = 1\n",
    "            elif (coord == 1):\n",
    "                x2 = x\n",
    "        elif ((not nzero) and (coord == 1)):\n",
    "            bboxes[cont][0] = [x1, y1]\n",
    "            bboxes[cont][1] = [x2, y2]\n",
    "            cont += 1\n",
    "            coord = 0\n",
    "            x1, y1, x2, y2 = 0, labeled.shape[0], 0, 0\n",
    "\n",
    "    bboxes = bboxes[0:cont]\n",
    "    return bboxes, cont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### crop_characters\n",
    "\n",
    "Given an image and character bounding boxes this function crops each character in an image and returns each character's corresponding binarized image in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_characters(img, bboxes, n):\n",
    "    characters = []\n",
    "    for i in range(0, n):\n",
    "        c = img.copy()[bboxes[i][0][1]:bboxes[i][1][1], bboxes[i][0][0]:bboxes[i][1][0]]\n",
    "        if (c.shape[0] != 0 and c.shape[1] != 0):\n",
    "            c = resize(c, (28, 28), mode='constant', cval=1.0, clip=True)\n",
    "            characters.append((c<0.80).reshape(784))\n",
    "    return characters, len(characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### labelsep\n",
    "\n",
    "Separates a full name label into a character list. Useful for the training part to have the labels of each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelsep(label):\n",
    "    if (type(label) is str or type(label) is np.str_):\n",
    "        decomposed_label = list(label)\n",
    "        labels = []\n",
    "        for i in range(0, len(decomposed_label)):\n",
    "            if (decomposed_label[i] != ' '):\n",
    "                labels.append(decomposed_label[i])\n",
    "        return labels\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_characters\n",
    "\n",
    "Given an image from the dataset and its label this function splits each character into one image and a label. The img_only variant doesn't return the labels (useful when testing with full names to save some memory and time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_characters(image, label):\n",
    "    labeled, nr_objects = get_labels(image)\n",
    "    bboxes, n = get_bboxes(labeled, nr_objects)\n",
    "    characters, n_chars = crop_characters(image, bboxes, n)\n",
    "    labels = labelsep(label)\n",
    "    return characters, labels[0:n_chars]\n",
    "\n",
    "def get_characters_img_only(image):\n",
    "    labeled, nr_objects = get_labels(image)\n",
    "    bboxes, n = get_bboxes(labeled, nr_objects)\n",
    "    characters, n_chars = crop_characters(image, bboxes, n)\n",
    "    return characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking results\n",
    "\n",
    "We can check if it extracts correctly all the data with these plots and prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = 1\n",
    "plt.imshow(dataset[selection], cmap='gray')\n",
    "plt.show()\n",
    "print(labels[selection])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters, charlabels = get_characters(dataset[selection], labels[selection])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(characters)):\n",
    "    plt.imshow(characters[i].reshape(28,28), cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "print(str(charlabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Defining our models\n",
    "\n",
    "We will define our four classifiers: MLP with RBM features, MLP with HOG features, MLP with PCA features and MLP only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define if we want to print status of the training process\n",
    "verbose_classifiers = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP with PCA features\n",
    "We define a MLP with 3 layers of 300, 400 and 150 neurons respectively to train it with the PCA features later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Classifier with PCA (Principal Component Analisis) features\n",
    "mlp_classifier_PCA = MLPClassifier(hidden_layer_sizes=(300,400,150), max_iter=5000, tol=0.0001, random_state=1, verbose=verbose_classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Preparing the train and test batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into train and test batches\n",
    "\n",
    "We divide the dataset into 80% train 20% test batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(dataset, labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train batch lenghts:\")\n",
    "print(\"X_train size: \"+str(len(X_train)))\n",
    "print(\"Y_train size: \"+str(len(Y_train)))\n",
    "print(\"\")\n",
    "print(\"Test batch lenghts:\")\n",
    "print(\"X_test size: \"+str(len(X_test)))\n",
    "print(\"Y_test size: \"+str(len(Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the train and test batches for characters\n",
    "\n",
    "We process the train and test batches to divide the batches into single character batches instead of full names, this is needed to train and will be useful in case of the test to test per character precision later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_chars = []\n",
    "Y_train_chars = []\n",
    "Train_with_inconsistencies = []\n",
    "z = 0\n",
    "for i in range(0, len(X_train)):\n",
    "    print_percentage(i*100/len(X_train), \"Processing train image \"+ str(i)+\" :\")\n",
    "    characters, charlabels = get_characters(X_train[i], Y_train[i])\n",
    "    if (len(characters) != len(charlabels) or len(characters) == 0 or len(charlabels) == 0):\n",
    "        if (enable_error_output):\n",
    "            print(\"[Warning] Input number \"+str(i)+\" inconsistent! Skipping this one...\")\n",
    "        Train_with_inconsistencies.append(i)\n",
    "        z += 1\n",
    "    else:\n",
    "        X_train_chars.extend(characters)\n",
    "        Y_train_chars.extend(charlabels)\n",
    "\n",
    "print_percentage(100, \"Processing train image \"+ str(len(X_train))+\" :\")\n",
    "print(\"\")\n",
    "print(str(100-(z*100/len(X_train)))+\"% of the data in train batch correctly extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Character splitted train batch lenghts:\")\n",
    "print(\"X_train_chars size: \"+str(len(X_train_chars)))\n",
    "print(\"Y_train_chars size: \"+str(len(Y_train_chars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_chars = []\n",
    "Y_test_chars = []\n",
    "Test_without_inconsistencies = []\n",
    "z = 0\n",
    "for i in range(0, len(X_test)):\n",
    "    print_percentage(i*100/len(X_test), \"Processing test image \"+ str(i)+\" :\")\n",
    "    characters, charlabels = get_characters(X_test[i], Y_test[i])\n",
    "    if (len(characters) != len(charlabels) or len(characters) == 0 or len(charlabels) == 0):\n",
    "        if (enable_error_output):\n",
    "            print(\"[Warning] Input number \"+str(i)+\" inconsistent! Skipping this one...\")\n",
    "        z += 1\n",
    "    else:\n",
    "        X_test_chars.extend(characters)\n",
    "        Y_test_chars.extend(charlabels)\n",
    "        Test_without_inconsistencies.append(i)\n",
    "\n",
    "print_percentage(100, \"Processing train image \"+ str(len(X_test))+\" :\")\n",
    "print(\"\")\n",
    "print(str(100-(z*100/len(X_test)))+\"% of the data in test batch correctly extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Character splitted test batch lenghts:\")\n",
    "print(\"X_test_chars size: \"+str(len(X_test_chars)))\n",
    "print(\"Y_test_chars size: \"+str(len(Y_test_chars)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the data to extract Principal Component Analysis features\n",
    "\n",
    "We transform the data to extract Principal Component Analysis features for the MLP_PCA classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardising the values\n",
    "Scaler = StandardScaler().fit(X_train_chars)\n",
    "X_pca_train = Scaler.transform(X_train_chars)\n",
    "# Call the PCA method with 100 components. \n",
    "pca = PCA(n_components=100)\n",
    "pca.fit(X_pca_train)\n",
    "P_train = pca.transform(X_pca_train)\n",
    "\n",
    "# Standardising the values\n",
    "X_pca_test = Scaler.transform(X_test_chars)\n",
    "\n",
    "# Call the PCA method with 100 components. \n",
    "P_test = pca.transform(X_pca_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also define a function in case we need to transform something later\n",
    "def PCA_transform(chars):\n",
    "    # Standardising the values\n",
    "    X_pca = Scaler.transform(chars)\n",
    "\n",
    "    # Call the PCA method with 100 components. \n",
    "    P = pca.transform(X_pca)\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the PCA looks like (only 2 PCA components):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_limit = 1000\n",
    "Target = Y_train_chars[:print_limit]\n",
    "Target_colors = Y_train_chars[:print_limit]\n",
    "for i in range(0, len(Target_colors)):\n",
    "    Target_colors[i] = ord(Target_colors[i])\n",
    "Characters = go.Scatter(\n",
    "    x = P_train[:print_limit,0],\n",
    "    y = P_train[:print_limit,1],\n",
    "    name = Target,\n",
    "    hoveron = Target,\n",
    "    mode = 'markers',\n",
    "    text = Target,\n",
    "    showlegend = True,\n",
    "    marker = dict(\n",
    "        size = 8,\n",
    "        color = Target_colors,\n",
    "        colorscale ='Jet',\n",
    "        showscale = False,\n",
    "        line = dict(\n",
    "            width = 2,\n",
    "            color = 'rgb(255, 255, 255)'\n",
    "        ),\n",
    "        opacity = 0.8\n",
    "    )\n",
    ")\n",
    "data = [Characters]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title= 'Principal Component Analysis (PCA)',\n",
    "    hovermode= 'closest',\n",
    "    xaxis= dict(\n",
    "         title= 'First Principal Component',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'Second Principal Component',\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    showlegend= True\n",
    ")\n",
    "\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "py.iplot(fig, filename='styled-scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/loading classifiers\n",
    "\n",
    "We feed the train characters and train character labels to each classifier or load the already trained classifiers from files (depending on the selected option):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not load_classifiers):\n",
    "    rbm_mlp_classifier.fit(X_train_chars[:], Y_train_chars)\n",
    "else:\n",
    "    rbm = joblib.load('RBM.pkl')\n",
    "    mlp = joblib.load('MLP_withRBMfeatures.pkl')\n",
    "    rbm_mlp_classifier = joblib.load('RBM_MLP_classifier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can define if we want to plot the components extracted by the RBM\n",
    "plot_rbm_features = True\n",
    "\n",
    "if (plot_rbm_features):\n",
    "    plt.figure(figsize=(28, 28))\n",
    "    for i, comp in enumerate(rbm.components_):\n",
    "        plt.subplot(30, 10, i+1)\n",
    "        plt.imshow(comp.reshape((28, 28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "    plt.suptitle('300 components extracted by the RBM', fontsize=16)\n",
    "    plt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not load_classifiers):\n",
    "    mlp_classifier_PCA.fit(P_train,Y_train_chars)\n",
    "else:\n",
    "    mlp_classifier_PCA = joblib.load('MLP_PCA.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the classifiers\n",
    "\n",
    "If enabled, this will save the trained models to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the classifiers if enabled:\n",
    "if (save_classifiers):\n",
    "    joblib.dump(rbm, 'RBM.pkl')\n",
    "    joblib.dump(mlp_classifier_PCA, 'MLP_PCA.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Testing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for testing results later\n",
    "\n",
    "predict_full_name given a full name image and a classifier divides the image into characters and asks the classifier to predict it, afterwards chains the predictions of each characters into a full string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform should be 'hog' for MLP_HOG classifier and 'pca' for MLP_PCA classifier, otherwise None\n",
    "def predict_full_name(name, classifier, transform=None):\n",
    "    characters = get_characters_img_only(name)\n",
    "    if (transform == 'pca'):\n",
    "        prediction = classifier.predict(PCA_transform(characters))\n",
    "    else:\n",
    "        prediction = classifier.predict(characters)\n",
    "    strg = ''\n",
    "    for i in range(0, len(prediction)):\n",
    "        strg = strg+prediction[i]\n",
    "    return strg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict_full_names gets the prediction for each consistent test data and returns the correct ratio and correlation ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform should be 'hog' for MLP_HOG classifier and 'pca' for MLP_PCA classifier, otherwise None\n",
    "def predict_full_names(classifier, transform=None):\n",
    "    correlation=0.0\n",
    "    correct = 0\n",
    "    for i in range(0,len(Test_without_inconsistencies)):\n",
    "        predicted_name = predict_full_name(X_test[Test_without_inconsistencies[i]], classifier, transform)\n",
    "        if (predicted_name == Y_test[Test_without_inconsistencies[i]]):\n",
    "            correct += 1\n",
    "        correlation += similar(predicted_name, Y_test[Test_without_inconsistencies[i]])\n",
    "        print_percentage(i*100/len(X_test),\"Making predictions \"+str(i)+\"/\"+str(len(X_test))+\":\")\n",
    "    print_percentage(100,\"Making predictions \"+str(len(X_test))+\"/\"+str(len(X_test))+\":\")\n",
    "    return (correct/len(Test_without_inconsistencies)), (correlation/len(Test_without_inconsistencies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "similar gives a correlation ratio between two strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(a,b):\n",
    "        return SequenceMatcher(None,a,b).ratio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing character recognintion\n",
    "\n",
    "We will test now individual character recognition scores for each classifier using the test batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm_mlp_prediction = rbm_mlp_classifier.predict(X_test_chars)\n",
    "mlp_prediction_PCA = mlp_classifier_PCA.predict(P_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MLP classification using RBM features:\\n%s\\n\" % (metrics.classification_report(Y_test_chars, rbm_mlp_prediction)))\n",
    "print(\"PCA + MLP classification:\\n%s\\n\" % (metrics.classification_report(Y_test_chars, mlp_prediction_PCA)))\n",
    "\n",
    "if (save_results):\n",
    "    result_output_file.write(\"MLP classification using RBM features:\\n%s\\n\" % (metrics.classification_report(Y_test_chars, rbm_mlp_prediction)))\n",
    "    result_output_file.write(\"\\nPCA + MLP classification:\\n%s\\n\" % (metrics.classification_report(Y_test_chars, mlp_prediction_PCA)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing full name recognition\n",
    "\n",
    "We will test now full name recognition scores using the consistent test batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_mlp_PCA, corr_mlp_PCA = predict_full_names(mlp_classifier_PCA, 'pca')\n",
    "correct_mlp, corr_mlp = predict_full_names(mlp_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Full name test results: \")\n",
    "print(\"========================================================================\")\n",
    "print(\"| Classifier            | Correct percentage      | Correlation ratio  |\")\n",
    "print(\"========================================================================\")\n",
    "print(\"| MLP with PCA features | \"+str(correct_mlp_PCA)+\"     | \"+str(corr_mlp_PCA)+\" |\")\n",
    "print(\"========================================================================\")\n",
    "\n",
    "if (save_results):\n",
    "    result_output_file.write(\"\\n\\nFull name test results: \")\n",
    "    result_output_file.write(\"\\n========================================================================\")\n",
    "    result_output_file.write(\"\\n| Classifier            | Correct percentage      | Correlation ratio  |\")\n",
    "    result_output_file.write(\"\\n========================================================================\")\n",
    "    result_output_file.write(\"\\n| MLP with PCA features | \"+str(correct_mlp_PCA)+\"     | \"+str(corr_mlp_PCA)+\" |\")\n",
    "    result_output_file.write(\"\\n========================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual testing\n",
    "\n",
    "We can see some predictions visually here by adding indexes to the \"indexes\" list and for each test data selected in \"indexes\" we will see the predictions of each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = [8, 10, 12, 14, 16]\n",
    "for ind in indexes:\n",
    "    mlp_pca_predict = predict_full_name(X_test[ind], mlp_classifier_PCA, 'pca')\n",
    "    \n",
    "    print(\"> Image:\")\n",
    "    plt.imshow(X_test[ind], cmap='gray')\n",
    "    plt.show()\n",
    "    print(\"MLP with PCA features predicted: \"+mlp_pca_predict)\n",
    "    print(\"\")\n",
    "    \n",
    "    if (save_results):\n",
    "        result_output_file.write(\"\\n\\n> Real label: \"+Y_test[ind])\n",
    "        result_output_file.write(\"\\nMLP with PCA features predicted: \"+mlp_pca_predict)\n",
    "        result_output_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (save_results):\n",
    "    result_output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving this trained model for further usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(mlp_classifier_PCA, 'name_recognizer.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from a different process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_recog = joblib.load('name_recognizer.pkl')\n",
    "img = cv2.imread('train1.jpg',0)\n",
    "img = delborders(img)\n",
    "print(type(img))\n",
    "plt.imshow(img,cmap='gray')\n",
    "plt.show()\n",
    "img = cv2.imread('train1.jpg',0)\n",
    "img = delborders(img)\n",
    "plt.imshow(img,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import try_all_threshold, threshold_isodata\n",
    "img = cv2.imread('train2.jpg',0)\n",
    "th = threshold_isodata(img, nbins=256, return_all=False)\n",
    "image = img > th\n",
    "plt.imshow(image,cmap='gray')\n",
    "plt.show()\n",
    "img2 = cv2.imread('train1.jpg',0)\n",
    "th2 = threshold_isodata(img2, nbins=256, return_all=False)\n",
    "image2 = img2 > th2\n",
    "plt.imshow(image2,cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For filtered image\n",
    "names_recog = joblib.load('name_recognizer.pkl')\n",
    "mlp_pca_predict = predict_full_name(image, names_recog, 'pca')\n",
    "print(mlp_pca_predict)\n",
    "\n",
    "# For filtered image\n",
    "mlp_pca_predict1 = predict_full_name(image2, names_recog, 'pca')\n",
    "print(mlp_pca_predict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
